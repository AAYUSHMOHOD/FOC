# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ROBd-7mAi1OO2q1-qqtSQHIaMtX-CsEK
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import nibabel as nib
import numpy as np
import pandas as pd

base_path = '/content/drive/My Drive/HarP_final/thresholded_segmentations/'

data = []

for split in ['train', 'test']:
    for stage in ['AD', 'CN', 'MCI']:
        folder_path = os.path.join(base_path, split, stage)
        for filename in os.listdir(folder_path):
            if filename.endswith(".nii") or filename.endswith(".nii.gz"):
                file_path = os.path.join(folder_path, filename)

                # Load NIfTI
                img = nib.load(file_path)
                data_array = img.get_fdata()

                # White voxels (non-zero)
                voxel_count = np.sum(data_array > 0)

                # Get voxel volume from header
                voxel_dims = img.header.get_zooms()[:3]  # x, y, z
                voxel_volume = np.prod(voxel_dims)

                # Total physical volume in mm³
                physical_volume = voxel_count * voxel_volume

                # Extract subject ID (adapt if needed)
                subject_id = filename.split('_')[0].split('.')[0]

                data.append({
                    "subject_id": subject_id,
                    "filepath": file_path,
                    "stage": stage,
                    "voxel_count": voxel_count,
                    "physical_volume_mm3": physical_volume
                })

# Create DataFrame
df = pd.DataFrame(data)

print(df.head)

# Save CSV
df.to_csv("/content/drive/My Drive/HarP_final/hippocampus_volumes_with_subjects.csv", index=False)

import pandas as pd
import re

# Load hippocampal volume dataset (with columns like: path, volume, stage, etc.)
volume_df = pd.read_csv('/content/drive/My Drive/HarP_final/hippocampus_volumes_with_subjects.csv')

# Load metadata
meta_df = pd.read_csv('/content/drive/My Drive/HarP_final/HarP_Dataset_4_19_2025.csv')  # <- Replace with actual filename

# Strip whitespace/formatting
meta_df['Subject'] = meta_df['Subject'].astype(str).str.strip()

# Extract subject ID from path like "ADNI_131_S_0691_38877_L.nii" -> "131_S_0691"
def extract_subject_id(path):
    match = re.search(r'ADNI_(\d+_S_\d+)', path)
    return match.group(1) if match else None

volume_df['Subject_ID'] = volume_df['filepath'].apply(extract_subject_id)

# Merge the metadata
merged_df = pd.merge(volume_df, meta_df, left_on='Subject_ID', right_on='Subject', how='left')

# View merged result
print(merged_df[['Subject_ID', 'stage', 'physical_volume_mm3', 'Sex', 'Age', 'Group']].head())

import pandas as pd
import numpy as np

def augment_to_target(df, label_col, target_size=2000, noise_std=10):
    current_size = df.shape[0]
    rows_needed = target_size - current_size

    # Estimate augmentations per row
    num_augmented_per_sample = max(1, int(np.ceil(rows_needed / current_size)))

    augmented_data = []
    for _, row in df.iterrows():
        for _ in range(num_augmented_per_sample):
            if len(augmented_data) >= rows_needed:
                break  # stop once we’ve added enough
            augmented_row = row.copy()
            augmented_row['physical_volume_mm3'] += np.random.normal(0, noise_std)
            augmented_row['Age'] += np.random.normal(0, 1)  # optional
            augmented_data.append(augmented_row)
        if len(augmented_data) >= rows_needed:
            break

    aug_df = pd.DataFrame(augmented_data)
    final_df = pd.concat([df, aug_df], ignore_index=True).sample(n=2000, random_state=42).reset_index(drop=True)
    return final_df

# Apply to your DataFrame
augmented_df = augment_to_target(merged_df, label_col='stage', target_size=2000)

print("New shape after augmentation:", augmented_df.shape)
print(augmented_df['stage'].value_counts())

from sklearn.preprocessing import LabelEncoder

# Initialize label encoder for 'Sex' and 'Group'
sex_encoder = LabelEncoder()
group_encoder = LabelEncoder()

# Encode 'Sex' (Male: 0, Female: 1)
augmented_df['Sex'] = sex_encoder.fit_transform(augmented_df['Sex'])

# Encode 'Group' (AD: 0, MCI: 1, CN: 2)
augmented_df['Group'] = group_encoder.fit_transform(augmented_df['Group'])

# Check the mappings
print("Sex Encoder Mappings:", dict(zip(sex_encoder.classes_, sex_encoder.transform(sex_encoder.classes_))))
print("Group Encoder Mappings:", dict(zip(group_encoder.classes_, group_encoder.transform(group_encoder.classes_))))

# View the result
print(augmented_df[['Subject_ID', 'Sex', 'Group']].head())

# Check for missing values
print(augmented_df.isnull().sum())

# Drop rows with missing values in important columns (you can also fill missing values)
#merged_df = merged_df.dropna(subset=['physical_volume_mm3', 'Age', 'Sex', 'Group'])

from sklearn.preprocessing import StandardScaler

# Initialize scaler
scaler = StandardScaler()

# Scale 'physical_volume_mm3' and 'Age'
augmented_df[['physical_volume_mm3', 'Age']] = scaler.fit_transform(augmented_df[['physical_volume_mm3', 'Age']])

# Check the scaled data
print(augmented_df[['physical_volume_mm3', 'Age']].head())

from sklearn.model_selection import train_test_split

# Features (X) - We'll use physical volume, age, and sex as features
X = augmented_df[['physical_volume_mm3', 'Age', 'Sex']]

# Target (y) - We're predicting 'Group' (AD, MCI, CN)
y = augmented_df['Group']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the size of train/test sets
print(f"Training data size: {X_train.shape[0]}")
print(f"Testing data size: {X_test.shape[0]}")

print(f"Dataset has {augmented_df.shape[0]} rows and {augmented_df.shape[1]} columns.")

!pip install xgboost
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Importing necessary classes

# Random Forest
rf = RandomForestClassifier(n_estimators=300, max_depth=8, random_state=42)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

# Gradient Boosting
gb = GradientBoostingClassifier(n_estimators=300, learning_rate=0.1, max_depth=5, random_state=42)
gb.fit(X_train, y_train)
gb_preds = gb.predict(X_test)

# XGBoost (optional, if installed)
xg = xgb.XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=5, use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xg.fit(X_train, y_train)
xg_preds = xg.predict(X_test)

target_names = [str(cls) for cls in le.classes_]

print("Random Forest:\n", classification_report(y_test, rf_preds, target_names=target_names))
print("Gradient Boosting:\n", classification_report(y_test, gb_preds, target_names=target_names))
print("XGBoost:\n", classification_report(y_test, xg_preds, target_names=target_names))

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import PolynomialFeatures, LabelEncoder # Import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# Polynomial + Scaling
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Assuming 'y' is already encoded, if not, encode it here
# If 'y' is not encoded, you should have a 'Group' column:
# y = LabelEncoder().fit_transform(augmented_df['Group'])

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=42, stratify=y)

# Base learners
estimators = [
    ('rf', RandomForestClassifier(n_estimators=200, max_depth=12, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)),
    ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42, use_label_encoder=False, eval_metric='mlogloss'))
]

# Stacking model
stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)

# Fit and Predict
stack.fit(X_train, y_train)
stack_preds = stack.predict(X_test)

# Get the LabelEncoder used earlier (or create a new one if 'y' was not previously encoded)
# ----> If you encoded 'y' earlier in the notebook, get that LabelEncoder instance:
# le = # ... get the LabelEncoder used for 'Group' column
# ----> If 'y' was not encoded, create a new LabelEncoder:
le = LabelEncoder().fit(augmented_df['Group']) # Assuming 'Group' is the original label column

# Report
print("Stacking Ensemble:\n", classification_report(y_test, stack_preds, target_names=[str(cls) for cls in le.classes_]))

import matplotlib.pyplot as plt
import seaborn as sns

# Random Forest feature importances
importances = rf.feature_importances_
features = X.columns
feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp.values, y=feat_imp.index)
plt.title("Feature Importances - Random Forest")

def predict_stage(model, poly, label_encoder, volume_mm3, age, gender_str):
    # Convert gender to numeric (assuming you used label encoding or one-hot)
    gender_numeric = 1 if gender_str.lower() == 'male' else 0

    # Create input as array in correct order of features
    input_features = np.array([[volume_mm3, age, gender_numeric]])

    # Apply polynomial features (same as training)
    input_poly = poly.transform(input_features)

    # Predict
    prediction = model.predict(input_poly)[0]

    # Get human-readable class
    predicted_stage = label_encoder.inverse_transform([prediction])[0]

    return predicted_stage

predicted = predict_stage(stack, poly, le, volume_mm3=3125.5, age=68, gender_str="Male")
print("Predicted Stage:", predicted)

print(le.classes_)
#'AD': np.int64(0), 'CN': np.int64(1), 'MCI': np.int64(2)

import joblib

# Save all required parts
joblib.dump((stack, poly, le), '/content/drive/My Drive/HarP_final/final_stacking_model.joblib')

stack_loaded, poly_loaded, le_loaded = joblib.load('/content/drive/My Drive/HarP_final/final_stacking_model.joblib')