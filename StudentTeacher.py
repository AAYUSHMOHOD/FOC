# -*- coding: utf-8 -*-
"""Another copy of working_3DUNET_ONE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cSZrWcuucj0Ej4KeS83xB0l-LWO4yAFr
"""

!pip install nibabel
!pip install -q torch_xla

import torch_xla
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.xla_multiprocessing as xmp

from torch_xla.distributed.parallel_loader import MpDeviceLoader

from google.colab import drive
drive.mount('/content/drive')

device = xm.xla_device()

import os    ####use this for aatttention
import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import nibabel as nib
import torch.nn.functional as F

class ADNINiftiDataset(Dataset):
    """Dataset for loading ADNI 3D Brain Scans and Corresponding Segmentations with Padding."""

    def __init__(self, root_dir, split="train", transform=None, num_channels=1):
        self.brain_dir = os.path.join(root_dir, "brain", split)
        self.seg_dir = os.path.join(root_dir, "segmentations", split)
        self.transform = transform
        self.num_channels = num_channels
        self.image_pairs = self._load_image_pairs()

    def _load_image_pairs(self):
        """Find all brain images and match them with their segmentation masks."""
        image_pairs = []
        for category in ["AD", "CN", "MCI"]:
            brain_path = os.path.join(self.brain_dir, category)
            seg_path = os.path.join(self.seg_dir, category)

            brain_images = sorted(glob.glob(os.path.join(brain_path, "*.nii")))
            for brain_img in brain_images:
                base_id = os.path.basename(brain_img).split("_MR_")[0]
                seg_candidates = glob.glob(os.path.join(seg_path, f"{base_id}_*.nii"))

                if len(seg_candidates) == 1:
                    seg_img = seg_candidates[0]
                    image_pairs.append((brain_img, seg_img))

        return image_pairs

    def load_nifti(self, file_path):
        """Load a NIfTI file and return as a NumPy array."""
        nifti_img = nib.load(file_path)
        return np.array(nifti_img.get_fdata(), dtype=np.float32)

    def pad_to_target_size(self, image, target_size=(128, 128, 64)):
        """Pads a 3D tensor symmetrically to the target size."""
        current_size = image.shape
        pad_h = target_size[0] - current_size[0]
        pad_w = target_size[1] - current_size[1]
        pad_d = target_size[2] - current_size[2]

        padding = (pad_d//2, pad_d - pad_d//2,  # Depth (front, back)
                   pad_w//2, pad_w - pad_w//2,  # Width (left, right)
                   pad_h//2, pad_h - pad_h//2)  # Height (top, bottom)

        return F.pad(image.unsqueeze(0), padding, mode='constant', value=0).squeeze(0)

    def __getitem__(self, idx):
        """Load, pad, and process brain + segmentation pair."""
        brain_path, seg_path = self.image_pairs[idx]

        brain = torch.tensor(self.load_nifti(brain_path))  # (H, W, T)
        seg = torch.tensor(self.load_nifti(seg_path))  # (H, W, T)

        # Apply padding to (256, 256, 189)
        brain = self.pad_to_target_size(brain)
        seg = self.pad_to_target_size(seg)

        # Rearrange to (C, H, W, T)
        brain = brain.unsqueeze(0).expand(self.num_channels, -1, -1, -1)  # (C, H, W, T)
        seg = seg.unsqueeze(0).expand(self.num_channels, -1, -1, -1)  # (C, H, W, T)

        if self.transform:
            brain = self.transform(brain)
            seg = self.transform(seg)

        return brain, seg  # Shape (C, H, W, T)

    def __len__(self):
        return len(self.image_pairs)

# Define collate function
def collate_fn(batch):
    """Custom collate function to ensure (B, C, H, W, T) format."""
    brains, segs = zip(*batch)
    brains = torch.stack(brains)  # (B, C, H, W, T)
    segs = torch.stack(segs)  # (B, C, H, W, T)
    return brains, segs

# Define DataLoader function
def get_adni_dataloader(root_dir, split="train", batch_size=1, num_workers=0, shuffle=True):
    dataset = ADNINiftiDataset(root_dir, split=split)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)
    return dataloader

import os
import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import nibabel as nib
import torch.nn.functional as F

def one_hot_encoding(seg, num_classes=2):
       """Converts segmentation labels to one-hot encoding."""
       # Cast to float32
       one_hot = F.one_hot(seg.long(), num_classes).permute(3, 0, 1, 2).type(torch.float32)  # (C, H, W, T)
       return one_hot

class ADNINiftiDataset(Dataset):
    def __init__(self, root_dir, split="train", transform=None):
        self.brain_dir = os.path.join(root_dir, "brain", split)
        self.seg_dir = os.path.join(root_dir, "segmentations", split)
        self.transform = transform
        self.image_pairs = self._load_image_pairs()

    def _load_image_pairs(self):
        image_pairs = []
        for category in ["AD", "CN", "MCI"]:
            brain_path = os.path.join(self.brain_dir, category)
            seg_path = os.path.join(self.seg_dir, category)

            brain_images = sorted(glob.glob(os.path.join(brain_path, "*.nii")))
            for brain_img in brain_images:
                base_id = os.path.basename(brain_img).split("_MR_")[0]
                seg_candidates = glob.glob(os.path.join(seg_path, f"{base_id}_*.nii"))
                if len(seg_candidates) == 1:
                    seg_img = seg_candidates[0]
                    image_pairs.append((brain_img, seg_img))
        return image_pairs

    def load_nifti(self, file_path):
        return torch.tensor(nib.load(file_path).get_fdata(), dtype=torch.float32)

    def pad_to_target_size(self, image, target_size=(128, 128, 64)):
        current_size = image.shape
        pad_h = target_size[0] - current_size[0]
        pad_w = target_size[1] - current_size[1]
        pad_d = target_size[2] - current_size[2]
        padding = (pad_d//2, pad_d - pad_d//2, pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2)
        return F.pad(image.unsqueeze(0), padding, mode='constant', value=0).squeeze(0)

    def __getitem__(self, idx):
        brain_path, seg_path = self.image_pairs[idx]
        brain = self.load_nifti(brain_path)  # (H, W, T)
        seg = self.load_nifti(seg_path)  # (H, W, T)
        brain = self.pad_to_target_size(brain).unsqueeze(0)  # (1, H, W, T)
        seg = self.pad_to_target_size(seg)  # (H, W, T)
        seg = one_hot_encoding(seg, num_classes=2)  # (2, H, W, T)
        if self.transform:
            brain = self.transform(brain)
            seg = self.transform(seg)
        return brain, seg

    def __len__(self):
        return len(self.image_pairs)

def collate_fn(batch):
    brains, segs = zip(*batch)
    brains = torch.stack(brains)  # (B, 1, 128, 128, 64)
    segs = torch.stack(segs)  # (B, 2, 128, 128, 64)
    return brains, segs

def get_adni_dataloader(root_dir, split="train", batch_size=1, num_workers=0, shuffle=True):
    dataset = ADNINiftiDataset(root_dir, split=split)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)
    return dataloader

import glob

root_dir = '/content/drive/My Drive/HarP_final/'

dataloader = get_adni_dataloader(root_dir, split="train", batch_size=1)
dataloader = MpDeviceLoader(dataloader, device)

brain_batch, seg_batch = next(iter(dataloader))

print(f"Brain Shape: {brain_batch.shape}, Segmentation Shape: {seg_batch.shape}")

import matplotlib.pyplot as plt
import numpy as np
from ipywidgets import interact, IntSlider

# Function to visualize slices
def visualize_slices(brain, seg):
    """Create an interactive slider to check slices across depth."""
    brain = brain.numpy()  # Convert tensors to NumPy
    seg = seg.numpy()

    depth = brain.shape[-1]  # Last dimension is depth (D)

    def plot_slice(slice_idx):
        fig, axes = plt.subplots(1, 2, figsize=(10, 5))

        axes[0].imshow(brain[0, :, :, slice_idx], cmap="gray")  # Brain slice
        axes[0].set_title(f"Brain Slice {slice_idx}")

        axes[1].imshow(seg[1, :, :, slice_idx], cmap="jet", alpha=0.7)  # Segmentation slice
        axes[1].set_title(f"Segmentation Slice {slice_idx}")

        plt.show()

    interact(plot_slice, slice_idx=IntSlider(min=0, max=depth-1, step=1, value=depth//2))

# Load dataset using DataLoader
dataloader = get_adni_dataloader(root_dir, split="train", batch_size=1, shuffle=True)
dataset_size = len(dataloader.dataset)
print(dataset_size)
# Fetch a single batch
brain_batch, seg_batch = next(iter(dataloader))  # Get one batch
brain = brain_batch[0]  # Extract first sample from batch
seg = seg_batch[0]      # Extract first sample from batch

# Print shape for verification
print(f"Brain Shape: {brain.shape}, Segmentation Shape: {seg.shape}")

# Visualize
visualize_slices(brain, seg)

import matplotlib.pyplot as plt    ####use this for attention
import numpy as np
from ipywidgets import interact, IntSlider

# Function to visualize slices
def visualize_slices(brain, seg):
    """Create an interactive slider to check slices across depth."""
    brain = brain.numpy()  # Convert tensors to NumPy
    seg = seg.numpy()

    depth = brain.shape[-1]  # Last dimension is depth (D)

    def plot_slice(slice_idx):
        fig, axes = plt.subplots(1, 2, figsize=(10, 5))

        axes[0].imshow(brain[0, :, :, slice_idx], cmap="gray")  # Brain slice
        axes[0].set_title(f"Brain Slice {slice_idx}")

        axes[1].imshow(seg[0, :, :, slice_idx], cmap="jet", alpha=0.7)  # Segmentation slice
        axes[1].set_title(f"Segmentation Slice {slice_idx}")

        plt.show()

    interact(plot_slice, slice_idx=IntSlider(min=0, max=depth-1, step=1, value=depth//2))

# Load dataset using DataLoader
dataloader = get_adni_dataloader(root_dir, split="train", batch_size=1, shuffle=True)
dataset_size = len(dataloader.dataset)
print(dataset_size)
# Fetch a single batch
brain_batch, seg_batch = next(iter(dataloader))  # Get one batch
brain = brain_batch[0]  # Extract first sample from batch
seg = seg_batch[0]      # Extract first sample from batch

# Print shape for verification
print(f"Brain Shape: {brain.shape}, Segmentation Shape: {seg.shape}")

# Visualize
visualize_slices(brain, seg)

import torch
import torch.nn as nn

class ADN(nn.Module):
    def __init__(self, in_channels):
        super(ADN, self).__init__()
        self.instance_norm = nn.InstanceNorm3d(in_channels, eps=1e-5, momentum=0.1, affine=False, track_running_stats=False)
        self.dropout = nn.Dropout(p=0.0)
        self.activation = nn.PReLU()

    def forward(self, x):
        return self.activation(self.dropout(self.instance_norm(x)))

class Convolution(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):
        super(Convolution, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)
        self.adn = ADN(out_channels)

    def forward(self, x):
        return self.adn(self.conv(x))

class SkipConnection(nn.Module):
    def __init__(self, submodule):
        super(SkipConnection, self).__init__()
        self.submodule = submodule

    def forward(self, x):
        return self.submodule(x)

class UNet(nn.Module):
    def __init__(self):
        super(UNet, self).__init__()

        self.encoder1 = Convolution(1, 32)
        self.encoder2 = SkipConnection(Convolution(32, 64))
        self.encoder3 = SkipConnection(Convolution(64, 128))
        self.encoder4 = SkipConnection(Convolution(128, 256))
        self.bottleneck = SkipConnection(Convolution(256, 512, stride=1))

        self.decoder4 = SkipConnection(nn.Sequential(
            nn.ConvTranspose3d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),
            ADN(256)
        ))
        self.decoder3 = SkipConnection(nn.Sequential(
            nn.ConvTranspose3d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            ADN(128)
        ))
        self.decoder2 = SkipConnection(nn.Sequential(
            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            ADN(64)
        ))
        self.decoder1 = nn.ConvTranspose3d(64, 2, kernel_size=3, stride=2, padding=1, output_padding=1)

    def forward(self, x):
        enc1 = self.encoder1(x)          # Output shape: (1, 32, 64, 64, 32)
        enc2 = self.encoder2(enc1)       # Output shape: (1, 64, 32, 32, 16)
        enc3 = self.encoder3(enc2)       # Output shape: (1, 128, 16, 16, 8)
        enc4 = self.encoder4(enc3)       # Output shape: (1, 256, 8, 8, 4)

        bottleneck = self.bottleneck(enc4)  # Output shape: (1, 512, 8, 8, 4)

        dec4 = self.decoder4(bottleneck)     # Output shape: (1, 256, 16, 16, 8)
        dec3 = self.decoder3(dec4)    # Output shape: (1, 128, 32, 32, 16)
        dec2 = self.decoder2(dec3)    # Output shape: (1, 64, 64, 64, 32)
        dec1 = self.decoder1(dec2)    # Output shape: (1, 2, 128, 128, 64)

        return dec1

# Create a model instance
model = UNet()

# Print the model architecture
#print(model)

# Example input tensor
input_tensor = torch.randn(1, 1, 128, 128, 64)  # Batch size of 1
output_tensor = model(input_tensor)
print("Output shape:", output_tensor.shape)  # Should be (1, 2, 128, 128, 64)

import torch
import numpy as np
import torch.nn as nn
class DiceLoss(nn.Module):
    def __init__(self, smooth=1.0):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, logits, targets):
        # Apply sigmoid if not already applied
        probs = torch.sigmoid(logits)

        # Flatten tensors
        probs = probs.view(-1)
        targets = targets.view(-1)

        intersection = (probs * targets).sum()
        dice_score = (2. * intersection + self.smooth) / (probs.sum() + targets.sum() + self.smooth)

        return 1 - dice_score


# Define the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#device = xm.xla_device()  # Get current TPU device
# Define the optimizer and loss function (Modify as needed)
#optimizer = torch.optim.Adam(model.parameters(),lr= 1e-3, weight_decay=1e-5, amsgrad=True)
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,
    betas=(0.9, 0.98),
    eps=1e-6,
    weight_decay=1e-5,
    amsgrad=True
)

#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)
loss_fn = torch.nn.BCEWithLogitsLoss()  # Example for segmentation tasks

def train_epoch(model, dataloader, device, accumulation_steps=2): # accumulation_steps = 2
    model.train()
    losses = []
    optimizer.zero_grad()  # Initialize gradients to zero before the epoch starts

    for i, (x, y) in enumerate(dataloader):
        x, y = x.to(device), y.to(device)

        out = model(x)
        loss = loss_fn(out, y)
        loss = loss / accumulation_steps  # Scale the loss by accumulation steps

        loss.backward()

        if (i + 1) % accumulation_steps == 0:  # Update every accumulation_steps iterations
            optimizer.step()
            optimizer.zero_grad()  # Reset gradients after update

        losses.append(loss.item() * accumulation_steps)  # Multiply loss back to original scale

    return np.mean(losses)

def validate_epoch(model, dataloader, device): # Pass device to validate_epoch
    model.eval()
    losses = []

    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)  # Use device here
            out = model(x)
            loss = loss_fn(out, y)
            losses.append(loss.item())

    return np.mean(losses)

# def train(model, train_loader, valid_loader, epochs=10, min_epochs=3, early_stop_count=5, device=xm.xla_device()): # Pass device to train
def train(model, train_loader, valid_loader, epochs=10, min_epochs=3, early_stop_count=5, device=device):
    best_valid_loss = float('inf')
    EARLY_STOP = early_stop_count

    for ep in range(epochs):
        train_loss = train_epoch(model, train_loader, device) # Pass device here
        print("TRAIN HAHAHAHAHHA--------------------------------------------------------------------------------")
        valid_loss = validate_epoch(model, valid_loader, device) # Pass device here

        print(f'Epoch {ep+1}: train_loss={train_loss:.5f}, valid_loss={valid_loss:.5f}----------------------------------------------')

        if ep > min_epochs:
            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                EARLY_STOP = early_stop_count  # Reset early stopping count
            else:
                EARLY_STOP -= 1
                if EARLY_STOP <= 0:
                    print("Early stopping triggered.")
                    return train_loss, valid_loss

    return train_loss, valid_loss

from torch.utils.data import Subset

# Load full dataset
full_train_dataset = ADNINiftiDataset(root_dir, split="train")
full_valid_dataset = ADNINiftiDataset(root_dir, split="test")

# Select the first 10 images for training
small_train_dataset = Subset(full_train_dataset, range(75))
small_train_loader = DataLoader(small_train_dataset, batch_size=1, shuffle=True)

# Select the first 3 images for validation
small_valid_dataset = Subset(full_valid_dataset, range(20))
small_valid_loader = DataLoader(small_valid_dataset, batch_size=1, shuffle=False)

dataset_size = len(small_train_loader.dataset)
dataset_size_V = len(small_valid_loader.dataset)
print(dataset_size , dataset_size_V)

#device = xm.xla_device()  # Get current TPU device
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
#x = x.type(torch.FloatTensor).to(device)
model.to(device)

for p in model.parameters():
    if p.dim() > 1:
            nn.init.kaiming_uniform_(p)

optimizer = torch.optim.AdamW(model.parameters(),lr= 1e-3, weight_decay=1e-5, amsgrad=True)
#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9) # Try SGD
loss_fn = nn.BCEWithLogitsLoss()


# Train the model on the small dataset
#train(model, small_train_loader, small_valid_loader, epochs=100, min_epochs=25, early_stop_count=10)
#torch.save('/content/drive/My Drive/HarP_final/teacher_model.pth')

import torch
import torch.nn as nn

class PatchEmbed3D(nn.Module):
    def __init__(self, in_channels=1, embed_dim=96, patch_size=4):
        super().__init__()
        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        return self.proj(x)

class SwinBlock(nn.Module):
    def __init__(self, dim, window_size=(4, 4, 4), num_heads=4):
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads

        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)

        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Linear(dim * 4, dim)
        )

    def window_partition(self, x):
        B, C, D, H, W = x.shape
        d, h, w = self.window_size
        assert D % d == 0 and H % h == 0 and W % w == 0, "Dimensions must be divisible by window size"

        x = x.view(B, C, D // d, d, H // h, h, W // w, w)
        x = x.permute(0, 2, 4, 6, 3, 5, 7, 1)  # (B, D//d, H//h, W//w, d, h, w, C)
        windows = x.contiguous().view(-1, d * h * w, C)  # (num_windows*B, N, C)
        return windows

    def window_reverse(self, windows, B, D, H, W):
        d, h, w = self.window_size
        C = self.dim
        x = windows.view(B, D // d, H // h, W // w, d, h, w, C)
        x = x.permute(0, 7, 1, 4, 2, 5, 3, 6).contiguous()
        x = x.view(B, C, D, H, W)
        return x

    def forward(self, x):
        B, C, D, H, W = x.shape
        x_windows = self.window_partition(x)  # (num_windows*B, N, C)

        x_windows = self.norm1(x_windows)
        attn_output, _ = self.attn(x_windows, x_windows, x_windows)  # (num_windows*B, N, C)
        x_windows = x_windows + attn_output

        x_windows = self.norm2(x_windows)
        x_windows = x_windows + self.mlp(x_windows)

        x = self.window_reverse(x_windows, B, D, H, W)
        return x

class SimpleSwinUNet3D(nn.Module):
    def __init__(self, in_channels=1, out_channels=2, embed_dim=96, patch_size=4, window_size=(4,4,4)):
        super().__init__()
        self.patch_embed = PatchEmbed3D(in_channels, embed_dim, patch_size=patch_size)
        self.swin_block = SwinBlock(embed_dim, window_size=window_size)

        self.decoder = nn.Sequential(
            nn.ConvTranspose3d(embed_dim, 64, kernel_size=patch_size, stride=patch_size),
            nn.ReLU(inplace=True),
            nn.Conv3d(64, out_channels, kernel_size=1)
        )

    def forward(self, x):
        x = self.patch_embed(x)       # Downsample
        x = self.swin_block(x)        # Windowed attention
        x = self.decoder(x)           # Upsample to original size
        return x

import torch
import torch.nn as nn
from einops import rearrange

class PatchEmbed3D(nn.Module):
    def __init__(self, in_channels=1, embed_dim=96, patch_size=4):
        super().__init__()
        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        return self.proj(x)

class WindowAttention3D(nn.Module):
    def __init__(self, dim, window_size=4, num_heads=4):
        super().__init__()
        self.window_size = window_size
        self.attn = nn.MultiheadAttention(dim, num_heads=num_heads, batch_first=True)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        B, C, D, H, W = x.shape
        ws = self.window_size
        x = rearrange(x, 'b c (d wd) (h wh) (w ww) -> (b d h w) (wd wh ww) c',
                      wd=ws, wh=ws, ww=ws)
        x = self.norm(x)
        x, _ = self.attn(x, x, x)
        x = rearrange(x, '(b d h w) (wd wh ww) c -> b c (d wd) (h wh) (w ww)',
                      b=B, d=D//ws, h=H//ws, w=W//ws, wd=ws, wh=ws, ww=ws)
        return x

class SwinBlock3D(nn.Module):
    def __init__(self, dim, window_size=4, num_heads=4):
        super().__init__()
        self.attn = WindowAttention3D(dim, window_size, num_heads)
        self.mlp = nn.Sequential(
            nn.Conv3d(dim, dim*4, kernel_size=1),
            nn.GELU(),
            nn.Conv3d(dim*4, dim, kernel_size=1),
        )
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        x = self.attn(x) + x
        x = self.mlp(x) + x
        return x

class PatchMerging3D(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.reduction = nn.Conv3d(in_dim, out_dim, kernel_size=2, stride=2)

    def forward(self, x):
        return self.reduction(x)

class PatchExpanding3D(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.up = nn.ConvTranspose3d(in_dim, out_dim, kernel_size=2, stride=2)

    def forward(self, x):
        return self.up(x)

class SwinUNet3D(nn.Module):
    def __init__(self, in_channels=1, out_channels=2, base_dim=48):
        super().__init__()
        self.embed = PatchEmbed3D(in_channels, base_dim)

        # Encoder
        self.encoder1 = SwinBlock3D(base_dim)
        self.down1 = PatchMerging3D(base_dim, base_dim*2)
        self.encoder2 = SwinBlock3D(base_dim*2)
        self.down2 = PatchMerging3D(base_dim*2, base_dim*4)
        self.encoder3 = SwinBlock3D(base_dim*4)

        # Bottleneck
        self.bottleneck = SwinBlock3D(base_dim*4)

        # Decoder
        self.up1 = PatchExpanding3D(base_dim*4, base_dim*2)
        self.decoder1 = SwinBlock3D(base_dim*2)
        self.up2 = PatchExpanding3D(base_dim*2, base_dim)
        self.decoder2 = SwinBlock3D(base_dim)

        # Final output layer
        self.out = nn.Conv3d(base_dim, out_channels, kernel_size=1)

    def forward(self, x):
        # Encoder
        x1 = self.embed(x)          # -> (B, base_dim, D/4, H/4, W/4)
        x2 = self.encoder1(x1)
        x3 = self.down1(x2)
        x4 = self.encoder2(x3)
        x5 = self.down2(x4)
        x6 = self.encoder3(x5)

        # Bottleneck
        x_bottleneck = self.bottleneck(x6)

        # Decoder
        x = self.up1(x_bottleneck)
        x = self.decoder1(x + x4)   # skip connection
        x = self.up2(x)
        x = self.decoder2(x + x2)   # skip connection

        return self.out(x)

import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, temperature=2.0, alpha=0.5):
    """
    KL divergence-based distillation loss.
    """
    student_soft = F.log_softmax(student_logits / temperature, dim=1)
    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)
    loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)
    return loss * alpha

def training_step(student_model, teacher_model, data, optimizer, criterion, alpha=0.5):
    student_model.train()
    teacher_model.eval()

    images, labels = data  # labels are ground truth segmentation maps
    with torch.no_grad():
        teacher_outputs = teacher_model(images)

    student_outputs = student_model(images)

    loss_ce = criterion(student_outputs, labels)  # hard target loss (CE or Dice)
    loss_kd = distillation_loss(student_outputs, teacher_outputs, temperature=2.0, alpha=alpha)

    loss = (1 - alpha) * loss_ce + loss_kd

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

import os
import glob
import torch
import numpy as np
from torch.utils.data import DataLoader, Subset
from torch.optim import Adam
from torch.nn import BCELoss, BCEWithLogitsLoss
from torch.optim.lr_scheduler import StepLR
import torch.nn as nn
import torch_xla
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl

# -------- Configuration --------
teacher_weights = '/content/drive/My Drive/HarP_final/my_trained_model_unet (1).pth'
save_dir = '/content/drive/My Drive/HarP_final/checkpoints/student/'
os.makedirs(save_dir, exist_ok=True)

num_epochs = 150
batch_size = 1
summary_steps = 10

# -------- Models --------
# Assuming SimpleSwinUNet3D is defined elsewhere
student = SimpleSwinUNet3D()
teacher = UNet()  # Define your UNet model appropriately

# Initialize XLA device (for TPU usage)
device = xm.xla_device()

# Load teacher model
teacher.load_state_dict(torch.load(teacher_weights, map_location='cpu'))
teacher.to(device)
teacher.eval()

# Move student model to device
student.to(device)

# Initialize student model weights
for p in student.parameters():
    if p.dim() > 1:
        nn.init.kaiming_uniform_(p)

# -------- Optimizer and Scheduler --------
optimizer = Adam(
    student.parameters(),
    lr=1e-3,
    betas=(0.9, 0.98),
    eps=1e-6,
    weight_decay=1e-5,
    amsgrad=True
)
scheduler = StepLR(optimizer, step_size=100, gamma=0.2)

# -------- Loss Function Setup --------
def is_sigmoid_output(model, sample_input):
    with torch.no_grad():
        output = model(sample_input)
    return output.min() >= 0 and output.max() <= 1

# Check if the teacher model uses sigmoid output or logits
sample_input = torch.randn(1, 1, 128, 128, 64).to(device)
if is_sigmoid_output(teacher, sample_input):
    print("Using BCELoss because teacher outputs probabilities.")
    loss_fn = BCELoss()
    use_sigmoid_on_student = True
else:
    print("Using BCEWithLogitsLoss because teacher outputs logits.")
    loss_fn = BCEWithLogitsLoss()
    use_sigmoid_on_student = False

# -------- Training Loop --------
def train_student(student, teacher, optimizer, train_loader):
    student.train()
    teacher.eval()
    summ = []

    for i, (img, gt) in enumerate(train_loader):
        img, gt = img.to(device), gt.to(device)

        # Teacher forward pass (no gradients)
        with torch.no_grad():
            teacher_output = teacher(img)
            teacher_output = torch.clamp(teacher_output, 0, 1)

        # Student forward pass
        student_output = student(img)

        # Apply sigmoid if necessary
        if use_sigmoid_on_student:
            student_output = torch.sigmoid(student_output)

        # Calculate loss
        loss = loss_fn(student_output, teacher_output)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if i % summary_steps == 0:
            pred = (student_output > 0.5).float()
            correct = (pred == gt).float().sum()
            accuracy = correct / gt.numel()
            summary = {'accuracy': accuracy.item(), 'loss': loss.item()}
            summ.append(summary)

        # Free memory (manual garbage collection)
        del img, gt, teacher_output, student_output, loss

    if summ:
        mean_accuracy = np.mean([x['accuracy'] for x in summ])
        mean_loss = np.mean([x['loss'] for x in summ])
        print(f'- Train metrics:\n\tAccuracy: {mean_accuracy:.4f}\n\tLoss: {mean_loss:.4f}')

# -------- Evaluation Loop --------
def evaluate(student, val_loader):
    student.eval()
    loss_summ = []
    accuracy_summ = []

    with torch.no_grad():
        for img, gt in val_loader:
            img, gt = img.to(device), gt.to(device)

            # Student forward pass
            output = student(img)

            # Apply sigmoid if necessary
            if use_sigmoid_on_student:
                output = torch.sigmoid(output)

            # Calculate loss
            loss = loss_fn(output, gt)
            loss_summ.append(loss.item())

            # Calculate accuracy
            pred = (output > 0.5).float()
            correct = (pred == gt).float().sum()
            accuracy = correct / gt.numel()
            accuracy_summ.append(accuracy.item())

            # Free memory (manual garbage collection)
            del img, gt, output, loss, pred

    # Compute and display average loss and accuracy
    mean_loss = np.mean(loss_summ)
    mean_accuracy = np.mean(accuracy_summ)
    print(f'- Eval metrics:\n\tAverage Loss: {mean_loss:.4f}\n\tAverage Accuracy: {mean_accuracy:.4f}')
    return mean_loss

# -------- Main Loop --------
if __name__ == "__main__":
    min_val_loss = float('inf')

    # -------- Dataset and DataLoader Setup --------
    root_dir = '/content/drive/My Drive/HarP_final/'
    full_train_dataset = ADNINiftiDataset(root_dir, split="train")
    full_valid_dataset = ADNINiftiDataset(root_dir, split="test")

    small_train_dataset = Subset(full_train_dataset, range(120))
    small_train_loader = DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True)

    small_valid_dataset = Subset(full_valid_dataset, range(20))
    small_valid_loader = DataLoader(small_valid_dataset, batch_size=batch_size, shuffle=False)

    # Wrap DataLoader with MpDeviceLoader for TPU
    train_loader = pl.MpDeviceLoader(small_train_loader, device)
    valid_loader = pl.MpDeviceLoader(small_valid_loader, device)

    for epoch in range(num_epochs):
        print(f'\n[Epoch {epoch+1}]')
        train_student(student, teacher, optimizer, train_loader)

        val_loss = evaluate(student, valid_loader)

        if val_loss < min_val_loss:
            min_val_loss = val_loss
            torch.save(student.state_dict(), f'{save_dir}/CP_best.pth')
            print('Saved new best model.')

        torch.save(student.state_dict(), f'{save_dir}/CP{epoch+1}.pth')
        print(f'Saved checkpoint for epoch {epoch+1}')

        scheduler.step()

import torch
import matplotlib.pyplot as plt
import numpy as np
from ipywidgets import interact, IntSlider


def visualize_one_validation_sample(model, valid_loader, device):
    """Visualize all slices for a single validation image with Ground Truth vs. Predictions."""
    model.eval()

    with torch.no_grad():
        x, y = next(iter(valid_loader))  # Get one batch (1 image)
        x, y = x.to(device), y.to(device)

        preds = torch.sigmoid(model(x))  # Convert logits to probabilities
        preds = (preds > 0.4).float()  # Apply thresholding

        # Convert tensors to numpy
        x_np = x[0, 0].cpu().numpy()  # Brain scan (128, 128, 64)
        y_np = y[0, 1].cpu().numpy()  # Ground truth mask (128, 128, 64) - Select the segmentation class
        preds_np = preds[0, 1].cpu().numpy()  # Predicted mask (128, 128, 64)

        depth = x_np.shape[-1]  # Get depth (D)

        # Function to plot a specific slice
        def plot_slice(slice_idx):
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            axes[0].imshow(x_np[:, :, slice_idx], cmap="gray")
            axes[0].set_title(f"Brain Slice {slice_idx}")

            axes[1].imshow(y_np[:, :, slice_idx], cmap="jet", alpha=0.7)
            axes[1].set_title(f"Ground Truth Mask {slice_idx}")

            axes[2].imshow(preds_np[:, :, slice_idx], cmap="jet", alpha=0.7)
            axes[2].set_title(f"Predicted Mask {slice_idx}")

            plt.show()

        # Create an interactive slider to navigate through slices
        interact(plot_slice, slice_idx=IntSlider(min=0, max=depth-1, step=1, value=depth//2))


visualize_one_validation_sample(student, small_valid_loader, device)